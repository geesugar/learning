# 3.2 数据清洗与去重

从原始数据源（尤其是网络爬虫数据）中获取的文本充满了噪声。数据清洗和去重的目标是提升数据质量，这对于训练出稳定、可靠且无偏见的模型至关重要。

## 1. 数据清洗 (Data Cleaning)

数据清洗是一个多阶段的过程，旨在移除无关和有害的内容。

*   **格式清洗**:
    *   **HTML/XML 标签移除**: 去除网页中的 `<div>`, `<span>` 等标签。
    *   **Markdown/Wiki 标记移除**: 清理格式化标记。
    *   **模板内容移除**: 删除网站页眉、页脚、导航栏、广告等重复性模板内容。通常使用启发式规则（如文本与标签的比例、链接密度）来识别和删除这些部分。

*   **低质量内容过滤**:
    *   **基于规则的过滤**: 
        *   句子长度过短或过长。
        *   单词平均长度异常。
        *   特殊符号或数字的比例过高。
        *   “lorem ipsum” 等占位符文本。
    *   **基于语言模型的过滤**: 使用一个小的、预先训练好的语言模型（如 fastText）来评估文本的困惑度 (Perplexity)。困惑度过高的文本通常被认为是低质量或非自然语言，应予以过滤。
    *   **语言识别**: 过滤掉非目标语言的文本。

*   **有害与不当内容移除**:
    *   **Pornographic and Offensive Content (NSFW)**: 使用关键词列表或专门的分类器来识别和移除色情、暴力、仇恨言论等内容。
    *   **个人身份信息 (Personally Identifiable Information, PII) 移除**: 检测并移除姓名、电话号码、邮箱地址、身份证号等敏感信息，保护用户隐私。

## 2. 数据去重 (Deduplication)

大规模网络数据中存在大量的重复或近乎重复的内容（如新闻稿、文章转载、代码片段）。去除这些冗余可以提高训练效率，并防止模型在评估时“看到”训练数据，从而导致评估结果虚高。

*   **精确去重**: 
    *   **方法**: 对文档或段落计算哈希值（如 SHA-256），哈希值相同的视为精确重复，直接删除。
    *   **应用**: 适用于去除完全一致的副本。

*   **模糊去重 (Fuzzy Deduplication)**:
    *   **目标**: 识别内容相似但不完全相同的近义重复文本。
    *   **常用技术**: **MinHash (最小哈希)** 结合 **LSH (Locality-Sensitive Hashing, 局部敏感哈希)**。
        1.  **Shingling**: 将文档切分为 k-grams (连续的 k 个词或字符)。
        2.  **MinHash**: 对每个文档的 k-grams 集合计算一个固定大小的“签名”（signature），这个签名可以快速估算两个文档的 Jaccard 相似度。
        3.  **LSH**: 将具有相似签名的文档分到同一个“桶”（bucket）中，从而避免了对所有文档对进行两两比较（`O(n^2)`），大大提高了效率。
        4.  **处理**: 对同一个桶内的文档进行精确的相似度比较，并移除重复项。

*   **去重级别**:
    *   **文档级别**: 移除完全或几乎一样的整个文档。
    *   **段落级别**: 在文档内部或文档之间，移除重复的段落。这对于清理代码或引文重复很有用。
