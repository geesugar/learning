# 3.3 数据预处理与 Tokenization

在数据清洗和去重之后，文本数据需要经过最后一步处理，才能被模型用于训练。这一步主要包括文本标准化和 Tokenization。

## 1. 文本标准化 (Text Normalization)

文本标准化的目标是减少文本的无意义变体，让模型能够更专注于学习语义内容。

*   **Unicode 标准化**: 将文本转换为标准的 Unicode 格式（如 NFC 或 NFKC），处理兼容字符和多样的表示法。
*   **大小写转换**: 通常将所有文本转换为小写，以减少词汇表的大小。但在某些情况下（如代码、命名实体识别），保留大小写信息可能是有益的。
*   **处理特殊符号**: 根据需求决定是否保留、替换或删除标点符号、表情符号等。

## 2. Tokenization

Tokenization 是将原始文本字符串分割成一个个小单元（Token）的过程。这些 Token 是模型词汇表（Vocabulary）中的基本单位。选择合适的 Tokenization 算法对模型性能、训练效率和处理未见词的能力至关重要。

### 子词 (Subword) Tokenization 算法

现代 LLM 普遍采用基于子词的 Tokenization 方法，它能在词汇表大小和序列长度之间取得很好的平衡。

*   **核心思想**: 将文本切分为比字符大、但比单词小的单元。常见词可以直接是一个 Token，而稀有词可以被拆分为多个有意义的子词片段。例如，“tokenization” 可能被拆分为 `["token", "ization"]`。
*   **优点**:
    *   **开放词汇表 (Open Vocabulary)**: 能够通过组合子词来表示任何词汇，有效处理未登录词 (Out-of-Vocabulary, OOV)。
    *   **压缩序列**: 相比字符级 Tokenization，序列长度更短，计算效率更高。
    *   **共享统计强度**: 相关的词（如 “run”, “running”）可以共享相同的子词 Token（如 “run”），使模型更容易学习它们的语义关联。

### 主流子词算法

*   **Byte-Pair Encoding (BPE)**:
    1.  **初始化**: 词汇表由所有单个字符组成。
    2.  **迭代合并**: 迭代地找出数据集中出现频率最高的一对相邻 Token，并将它们合并成一个新的 Token，加入词汇表。
    3.  **终止**: 当达到预设的词汇表大小或没有更多的合并可以进行时，停止迭代。
    *   **应用**: GPT 系列模型使用此算法。

*   **WordPiece**:
    *   **原理**: 与 BPE 类似，也是通过迭代合并来构建词汇表。但其合并标准不是基于频率，而是基于合并后能最大化训练数据似然（Likelihood）的原则。
    *   **应用**: BERT 和 DistilBERT 使用此算法。

*   **SentencePiece**:
    *   **特点**: 将所有文本（包括空格）都视为普通符号序列进行处理。它将空格用一个特殊符号（如 `_`）表示，从而使得 Tokenization 过程完全可逆，并且不依赖于特定语言的分词规则。
    *   **算法**: SentencePiece 是一个库，它实现了 BPE 和 Unigram 两种算法。
    *   **应用**: Llama, T5, mBART 等多语言模型广泛采用。

### 词汇表 (Vocabulary)

词汇表是所有唯一 Token 的集合。在 Tokenization 之后，文本序列会被转换成一个整数序列，每个整数代表其在词汇表中的索引。词汇表的大小是一个重要的超参数，需要权衡模型的表达能力和计算效率。
