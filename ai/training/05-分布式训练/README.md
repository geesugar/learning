# 第 5 章：分布式训练

随着模型和数据规模的急剧增长，分布式训练已成为必要手段。本章介绍主流的并行训练技术和框架，解释如何利用多台设备协同训练一个巨大的模型。

## 学习内容

1.  [**并行技术概述**](./01-并行技术概述.md)
    *   通过图解理解三种基本的并行策略：数据并行、流水线并行和张量并行，并了解它们的优缺点。

2.  [**ZeRO 内存优化技术**](./02-ZeRO内存优化技术.md)
    *   深入学习 DeepSpeed 的核心创新 ZeRO，理解其如何通过分片存储来大幅降低单个 GPU 的显存消耗。

3.  [**主流框架与实践**](./03-主流框架与实践.md)
    *   了解并对比当前主流的分布式训练框架：DeepSpeed、Megatron-LM 和 PyTorch FSDP，知道在不同场景下如何选择合适的工具。