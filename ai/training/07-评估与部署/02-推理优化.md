# 7.2 推理优化

推理（Inference）是将训练好的模型用于生成文本的实际过程。对于大语言模型，推理可能非常耗时且消耗大量计算资源。推理优化的目标是在保证输出质量的前提下，降低延迟、提高吞吐量、减少显存占用。

## 1. 核心优化技术

*   **KV 缓存 (KV Cache)**:
    *   **问题**: 在自回归生成文本时，每生成一个新 token，模型都需要对整个序列（包含所有已生成的 token）重新计算注意力。这导致计算量随着生成长度的增加而急剧增长。
    *   **优化**: Transformer 的 Decoder 在计算注意力时，每个 token 的键（Key）和值（Value）向量是固定的。KV 缓存技术将这些计算过的 K 和 V 向量存储在 GPU 显存中，在生成下一个 token 时直接复用，而无需重新计算，从而将注意力计算的复杂度从 `O(n^2)` 降低到 `O(n)`。这是 LLM 推理中最基本、最重要的优化。

*   **模型量化 (Quantization)**:
    *   **思想**: 使用更低精度的数据类型（如 INT8, INT4）来表示模型的权重，甚至激活值，以替代标准的 FP32 或 FP16。
    *   **优点**: 
        *   **减少显存占用**: INT8 权重占用的显存是 FP16 的一半，INT4 则更少。
        *   **加速计算**: 许多现代硬件为低精度整数运算提供了专门的加速单元。
    *   **方法**: AWQ, GPTQ, GGML/GGUF 是目前主流的“仅权重”量化方案，可以在几乎不损失模型性能的情况下，实现显著的压缩和加速。

*   **Batching (批处理)**:
    *   **思想**: 将多个独立的推理请求打包成一个批次（batch），然后一次性送入 GPU 进行处理。这可以更好地利用 GPU 的并行计算能力，大幅提高吞吐量（每秒处理的 token 数）。
    *   **挑战**: 不同请求的序列长度不同，会导致批次中较短的序列需要等待较长的序列完成，产生浪费。**Continuous Batching**（连续批处理，被 vLLM 等框架采用）通过更动态的请求调度和内存管理，解决了这个问题。

*   **投机采样 (Speculative Decoding)**:
    *   **思想**: 用一个小的、快速的“草稿”模型来一次性生成一小段文本草稿，然后用大的、准确的目标模型来一次性验证整个草稿。如果验证通过，就一次性接受多个 token，从而加速生成过程。

## 2. 其他技术

*   **模型剪枝 (Pruning)**: 移除模型中被认为是“不重要”的权重或连接，以减小模型大小和计算量。
*   **知识蒸馏 (Knowledge Distillation)**: 用一个大的“教师”模型来训练一个小的“学生”模型，让学生模型学习模仿教师模型的输出。这可以有效地将大模型的能力迁移到更适合部署的小模型上。
*   **FlashAttention**: 在推理和训练中都极为重要。通过优化 GPU 内存的读写方式，在不改变数学等价性的前提下，大幅加速注意力计算。
