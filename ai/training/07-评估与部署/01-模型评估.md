# 7.1 模型评估

模型评估是衡量一个大语言模型能力、发现其优势和缺陷、指导后续优化方向的关键环节。由于语言任务的复杂性和开放性，LLM 的评估是一个多维度、多层次的系统工程。

## 1. 评测基准与数据集

评测基准通常由一系列精心设计的数据集组成，每个数据集关注模型的一种或多种特定能力。

*   **综合与常识推理**:
    *   **MMLU (Massive Multitask Language Understanding)**: 一个非常全面的基准，涵盖了 57 个不同领域的任务（包括初等数学、美国历史、计算机科学、法律等），是衡量模型知识广度和问题解决能力的核心指标。
    *   **HellaSwag**: 评估模型的常识推理能力。任务是在给定一个情境后，从四个选项中选择最合理的续写。
    *   **ARC (AI2 Reasoning Challenge)**: 包含一系列需要科学常识才能回答的复杂问题。

*   **数学与代码能力**:
    *   **GSM8K (Grade School Math)**: 包含数千个小学水平的数学应用题，旨在评估模型的数学推理和计算能力。
    *   **HumanEval**: 由 OpenAI 发布，包含一系列编程问题，用于评估模型根据文档字符串生成正确 Python 代码的能力。
    *   **MBPP (Mostly Basic Python Programming)**: 另一个代码生成基准，专注于基础的 Python 编程任务。

*   **知识与问答**:
    *   **TriviaQA**: 一个大规模的知识问答数据集。
    *   **Natural Questions**: 基于真实 Google 搜索查询的问答数据集。

*   **安全性与对齐**:
    *   **TruthfulQA**: 旨在评估模型生成答案的真实性，检测模型是否会复述网络上的错误信息。
    *   **ToxiGen**: 用于衡量模型产生有害或冒犯性言论的倾向。

## 2. 评估方法

*   **自动化评估 (Automated Evaluation)**:
    *   **优点**: 成本低、速度快、可重复性高。
    *   **方法**: 将模型的输出与标准的参考答案进行比较。对于选择题或代码生成等有明确答案的任务，可以直接判断对错。对于开放式生成任务，则使用 ROUGE, BLEU 等基于 n-gram 重叠度的指标（这些指标在评估 LLM 创造性任务时局限性较大）。
    *   **局限性**: 难以评估答案的语义一致性、逻辑正确性或创造性。

*   **人工评估 (Human Evaluation)**:
    *   **优点**: 是评估模型输出质量的“金标准”，尤其是在评估对话质量、内容有用性和安全性等主观指标时。
    *   **方法**: 由人类评估员根据一系列标准（如：是否有用、是否真实、是否无害）对模型的输出进行打分或排序。
    *   **局限性**: 成本高、耗时长、主观性强、难以规模化。

*   **基于模型的评估 (Model-based Evaluation)**:
    *   **思想**: 使用一个强大的“裁判”模型（如 GPT-4）来评估目标模型的输出。让裁判模型对两个不同模型的回答进行比较和打分。
    *   **代表**: **MT-Bench** 和 **AlpacaEval** 是这类评估方法的典型代表，它们提供了一套标准的评测流程和强大的裁判模型，实现了在开放式问答上低成本、可规模化的评估。

*   **对抗性测试 (Adversarial Testing)**:
    *   **目标**: 主动寻找模型的弱点和漏洞，特别是在安全性方面。
    *   **方法**: 人类专家（或自动化工具）精心设计各种“红队提示”（Red Teaming Prompts），试图诱导模型产生有害、违规或错误的输出。
