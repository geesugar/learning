# 7.3 服务部署

服务部署是将优化后的模型封装成一个稳定、可扩展的在线服务，通过 API 等形式供应用程序调用的最后一步。

## 1. 推理引擎 (Inference Engines)

推理引擎是专门为高效执行深度学习模型而设计的软件库。它们集成了多种推理优化技术，并提供了易于使用的接口。

*   **vLLM**:
    *   **特点**: 一个非常流行的高性能 LLM 推理和服务引擎。其核心创新是 **PagedAttention**，一种借鉴了操作系统中虚拟内存和分页思想的注意力算法，可以非常高效地管理 KV 缓存，显著减少内存浪费和碎片。
    *   **优点**: 吞吐量极高，支持连续批处理（Continuous Batching）和流式生成，已成为许多开源项目的首选推理后端。

*   **TensorRT-LLM (NVIDIA)**:
    *   **特点**: NVIDIA 官方推出的 LLM 推理优化库。它将模型编译成一个高度优化的引擎，充分利用 NVIDIA GPU 的硬件特性。
    *   **优点**: 通常能在 NVIDIA 硬件上达到极致的低延迟和高性能。支持 in-flight batching（类似于连续批处理）。

*   **Text Generation Inference (TGI, by Hugging Face)**:
    *   **特点**: Hugging Face 官方推出的推理服务器，专为部署 `transformers` 模型设计。
    *   **优点**: 与 Hugging Face 生态无缝集成，支持多种优化技术，易于部署。

## 2. API 服务化

将模型部署为服务，通常意味着将其封装在一个 Web 服务器中，通过 HTTP API 对外提供服务。

*   **常见的 API 框架**: 
    *   **FastAPI**: 基于 Python 的现代、高性能 Web 框架，非常适合构建 AI/ML 服务的 API。
    *   **Flask**: 一个轻量级的 Python Web 框架，也常用于快速搭建模型 API。

*   **OpenAI 兼容 API**: 
    *   许多开源推理服务器（如 vLLM, TGI）都提供了与 OpenAI API 格式兼容的接口。这意味着，你可以用一个开源模型替换掉对 GPT-4 的 API 调用，而无需修改客户端代码，极大地便利了模型的切换和测试。

## 3. 关键服务特性

*   **流式生成 (Streaming)**:
    *   对于聊天等交互式应用，用户不希望等待模型生成完整个回答后才看到结果。流式生成允许模型每生成一个或几个 token，就立刻将其返回给客户端，提供了更好的用户体验。

*   **分布式推理 (Distributed Inference)**:
    *   对于无法放入单张 GPU 的超大模型，需要使用张量并行（Tensor Parallelism）等技术，将模型切分到多张 GPU 或多台机器上进行协同推理。

*   **监控与扩展**: 
    *   在生产环境中，需要对模型的 QPS（每秒查询数）、延迟、GPU 利用率等指标进行监控。
    *   通常使用 Kubernetes 等容器编排工具来管理模型的部署，并根据负载自动进行水平扩展（增加或减少模型实例的数量）。
