# 0.2 发展简史与生态

大语言模型的发展并非一蹴而就，它建立在数十年的 NLP 研究和计算能力突破之上。尤其是 2017 年 Transformer 架构的提出，为 LLM 的诞生奠定了基石。

## 发展里程碑

1.  **前 Transformer 时代 (Pre-Transformer Era)**
    *   **RNN/LSTM**: 循环神经网络及其变体 LSTM 是处理序列数据的主流模型，但受限于长距离依赖问题和并行计算能力。

2.  **Transformer 的诞生 (2017)**
    *   **论文《Attention Is All You Need》**: Google 的研究人员提出了 Transformer 架构，完全抛弃了循环结构，仅依赖自注意力机制。这使得模型能够捕捉全局依赖，并且极大地提升了并行计算的效率，为训练更大规模的模型打开了大门。

3.  **预训练模型的兴起 (2018-2020)**
    *   **BERT (Google, 2018)**: 引入了掩码语言建模（MLM）和双向上下文理解，成为理解任务的强大基石。
    *   **GPT-2 (OpenAI, 2019)**: 展示了 Decoder-Only 架构在文本生成方面的巨大潜力，其生成的文本连贯、流畅，令人惊艳。
    *   **T5 (Google, 2019)**: 提出了一个统一的文本到文本（Text-to-Text）框架，将所有 NLP 任务都转化为生成任务。

4.  **“大”模型时代与涌现能力 (2020-至今)**
    *   **GPT-3 (OpenAI, 2020)**: 拥有 1750 亿参数，首次清晰地展示了 LLM 的“涌现能力”，特别是上下文学习（In-context Learning），引爆了整个领域。
    *   **PaLM (Google, 2022)**: 进一步将模型规模推向 5400 亿参数，并在思维链推理等方面取得了突破。
    *   **ChatGPT (OpenAI, 2022)**: 通过指令微调和 RLHF 对模型进行对齐，创造了现象级的产品。它让世界看到了 LLM 作为通用 AI 助手的巨大潜力，开启了生成式 AI 的新纪元。

## 开源与闭源生态

随着 LLM 的发展，逐渐形成了两大阵营：

*   **闭源模型 (Closed-source Models)**:
    *   **代表**: OpenAI (GPT-4), Google (Gemini), Anthropic (Claude)。
    *   **特点**: 通常是技术上最领先的模型，但只能通过 API 访问，不开放模型权重和训练细节。用户按量付费使用。
    *   **优势**: 性能强大，开箱即用，无需关心底层部署和维护。
    *   **劣势**: 成本高，数据隐私存疑，技术黑盒，依赖于服务提供商。

*   **开源模型 (Open-source Models)**:
    *   **代表**: Llama 系列 (Meta), Mistral/Mixtral (Mistral AI), Falcon (TII), Qwen (Alibaba)。
    *   **特点**: 公开模型权重，允许研究人员和企业在本地部署、修改和微调。
    *   **优势**: 免费、透明、数据可控、可深度定制，催生了繁荣的社区和生态系统。
    *   **劣势**: 通常性能略逊于最顶尖的闭源模型，需要自行承担部署和维护的成本与技术挑战。

当前的格局是，闭源模型在能力上限上持续引领，而开源模型在快速追赶，并在特定领域或经过精细微调后，能够达到甚至超过闭源模型的性能，为广大开发者和中小企业提供了普惠的 AI 能力。
