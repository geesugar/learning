# 6.1 指令微调 (Supervised Fine-tuning, SFT)

指令微调（也常被称为“对齐”的第一步）是让一个已经完成预训练的基础模型（Base Model）学会理解并遵循人类指令的关键过程。其目标是让模型从一个单纯的文本补全工具，转变为一个有用的、可交互的助手。

*   **核心思想**: 使用高质量的“指令-回答”数据对，以监督学习（类似于 Causal LM）的方式微调模型，教会模型在看到特定指令时，应该生成什么样的回答。

## 数据集构建

SFT 的成功在很大程度上依赖于指令数据集的质量、数量和多样性。

*   **开源指令数据集**: 
    *   **Alpaca**: 斯坦福大学发布的经典数据集，包含约 52k 条由 GPT-3.5 (text-davinci-003) 生成的指令-回答对。
    *   **Dolly**: 由 Databricks 员工众包创建，强调数据是由人类真实编写的。
    *   **Open-Orca**: 一个包含数百万条 GPT-4 级别响应的大规模、高质量指令数据集。
*   **数据生成方法**: 
    *   **Self-Instruct**: 一种利用现有强大模型（如 GPT-4）来自动生成新指令、输入和回答的方法，可以低成本地扩充数据集。
*   **数据质量**: 相比于数量，指令的多样性（覆盖不同任务类型）和回答的质量（有用、无害、真实）更为重要。

### 数据集格式示例

不同的项目可能使用不同的格式，但通常会包含指令（instruction）、可选的上下文（input）和期望的输出（output）。Alpaca 数据集的格式是一个很好的例子：

```json
[
  {
    "instruction": "Tell me about the history of the internet.",
    "input": "",
    "output": "The internet began as ARPANET, a US government project in the 1960s..."
  },
  {
    "instruction": "Summarize the following text.",
    "input": "The quick brown fox jumps over the lazy dog. This sentence contains all the letters of the English alphabet.",
    "output": "The provided text is a pangram, a sentence that includes every letter of the alphabet."
  }
]
```

在训练前，这些字段通常会被格式化成一个统一的 Prompt 模板。

## 训练细节

*   **Prompt 模板**: 为了让模型能区分指令、上下文和回答，需要将数据格式化为固定的模板，例如：
    ```
    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

    ### Instruction:
    {instruction}

    ### Input:
    {input}

    ### Response:
    {output}
    ```

*   **损失计算 (Loss Masking)**:
    *   **目标**: 我们只希望模型因为生成了错误的“回答”而受到惩罚，而不关心它如何处理“指令”部分。换句话说，模型只需要学会生成 `Response`，而不需要学会生成 `Instruction` 或 `Input`。
    *   **实现**: 在计算交叉熵损失时，将 Prompt 模板中 `Instruction` 和 `Input` 部分对应的 Token 的标签（labels）设置为一个特殊值（如 -100）。PyTorch 的交叉熵损失函数会自动忽略这些值为 -100 的标签，从而只对 `Response` 部分的 Token 计算损失。

*   **超参数**: SFT 通常使用比预训练小得多的学习率，并且训练的 epoch 数量也较少（通常 1-3 个 epoch 即可），以避免模型遗忘在预训练阶段学到的通用知识（这个现象被称为“灾难性遗忘”）。