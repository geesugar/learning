# 6.4 微调技术对比

在将基础模型适配到特定任务或提升其有用性时，有多种微调策略可供选择。它们在性能、成本和应用场景上各有取舍。这里我们主要对比三种主流路径：全量微调 (Full Fine-tuning)、参数高效微调 (PEFT, 以 LoRA 为例) 和人类反馈强化学习 (RLHF)。

## 对比表格

| 特性 | 全量微调 (Full Fine-tuning) | PEFT (以 LoRA 为例) | RLHF (Reinforcement Learning from Human Feedback) |
| :--- | :--- | :--- | :--- |
| **核心思想** | 更新模型的所有参数。 | 冻结大部分参数，仅更新少量额外添加的参数。 | 使用强化学习，通过人类偏好数据训练的奖励模型来优化模型。 |
| **性能/效果** | **最高**。理论上能达到最佳性能，因为所有参数都参与了适配。 | **接近全量微调**。在很多任务上能以极小的性能损失逼近全量微调的效果。 | **优化“对齐”而非“知识”**。不旨在教授新知识，而是让模型的行为更符合人类偏好（更有用、更无害）。 |
| **计算成本** | **非常高**。需要大量 GPU 显存和计算时间。 | **非常低**。训练速度快，显存占用小，消费级 GPU 即可完成。 | **极高**。流程最复杂，涉及多个模型的训练（SFT、RM、PPO），成本最高。 |
| **存储成本** | **高**。每微调一个任务，就需要保存一份完整的模型副本。 | **极低**。只需保存微小的适配器权重（几 MB），多个任务可以共享同一个基础模型。 | **高**。需要存储 SFT、RM 和最终模型等多个版本的权重。 |
| **数据需求** | 需要大量高质量的“指令-回答”对。 | 与全量微调类似，但对数据量的容忍度稍高。 | 需要**偏好数据**（如：回答 A 比回答 B 好），这种数据通常比简单的问答对更难获取。 |
| **实现复杂度** | **简单**。流程直接，与预训练类似。 | **简单**。使用 `peft` 等库，只需几行代码即可应用。 | **非常复杂**。涉及强化学习算法（PPO），训练不稳定，调试困难。 |

---

## 适用场景分析

### 1. 全量微调 (Full Fine-tuning)

*   **何时使用**: 
    *   当追求**极致的性能**，且计算资源充足时。
    *   当目标任务与预训练任务差异巨大，需要模型进行深度调整时。
    *   当需要将大量**新的领域知识**注入模型时（例如，用海量医学文献微调一个通用模型，使其成为医学专家）。
*   **例子**: 为金融领域训练一个专有模型，使用公司内部的海量、高质量金融文档进行微调。

### 2. PEFT (以 LoRA 为例)

*   **何时使用**: 
    *   当**计算资源有限**时（这是最常见的场景）。
    *   当需要为**多个不同任务**微调同一个基础模型时。只需为每个任务训练和存储一个轻量的 LoRA 适配器即可，极大节约了成本。
    *   当希望快速迭代和实验不同的微调数据或超参数时。
    *   作为一种服务，为大量用户提供个性化的模型微调能力。
*   **例子**: 为一个聊天机器人增加多种“性格”或“技能”，每个技能都是一个独立的 LoRA 适配器，可以按需加载。

### 3. RLHF

*   **何时使用**: 
    *   当模型的**核心知识已经足够**（通过预训练或 SFT），但其**行为不符合期望**时。
    *   当目标是提升模型的“对齐”水平，使其更安全、更有用、更能拒绝不当请求时。
    *   当评估标准难以用简单的文本匹配来衡量，而更多地依赖于人类的主观感受时（例如，一个回答是否“有帮助”或“有创意”）。
*   **例子**: ChatGPT, Claude 等先进聊天机器人的最后一步优化。它们通过 RLHF 学会了如何更好地进行多轮对话、承认错误、拒绝有害问题，使其行为更像一个负责任的 AI 助手，而不是一个单纯的知识库。

## 总结

*   **SFT / PEFT** 主要解决**“能不能做”**的问题（教授知识和技能）。
*   **RLHF** 主要解决**“该不该这么做”**的问题（规范行为和价值观）。

在实践中，这三者常常结合使用：先通过**全量微调或 PEFT (SFT)** 让模型掌握特定任务的能力，然后通过 **RLHF** 让模型的表达方式和行为准则更符合人类的期望。
