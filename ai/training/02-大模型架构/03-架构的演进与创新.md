# 2.3 架构的演进与创新

随着模型规模的不断增大和应用场景的日益复杂，研究人员对基础的 Transformer 架构进行了大量的改进和创新，以提升其效率、性能和能力。

### 1. 注意力机制的优化

标准的自注意力机制计算复杂度与序列长度的平方成正比（`O(n^2)`），这使得处理长序列（如长文档、整本书）的成本极高。

*   **稀疏注意力 (Sparse Attention)**: 思想是每个词不需要与所有其他词计算注意力，而只与部分“重要”的词计算。代表方法有 Longformer、BigBird，它们结合了局部注意力和全局注意力。
*   **线性注意力 (Linear Attention)**: 尝试将计算复杂度从 `O(n^2)` 降低到 `O(n)`，从而在理论上实现对无限长序列的处理。代表方法有 Linformer、Performer。
*   **FlashAttention**: 一种不改变理论计算复杂度，但通过优化 GPU 内存读写（I/O-aware）来大幅加速注意力计算和减少显存占用的技术，已成为现代 LLM 训练和推理的标准配置。

### 2. 混合专家模型 (Mixture-of-Experts, MoE)

MoE 是一种扩展模型参数规模而不显著增加计算量的有效方法。

*   **核心思想**: 将模型中的某些层（通常是前馈网络 FFN 层）替换为多个“专家”子网络。对于每个输入，一个可学习的“门控网络”（Gating Network）会动态地选择激活一小部分专家（通常是 1-2 个）来处理该输入。
*   **优点**:
    *   可以在总参数量极大的情况下（如万亿级别），保持每次前向传播的计算量（FLOPs）相对较小。
    *   每个专家可以特化学习不同方面的知识。
*   **代表模型**: Mixtral 8x7B, GPT-4 (据推测)。
*   **挑战**: 训练不稳定、负载均衡、通信开销大。

### 3. 长上下文处理技术 (Long Context)

提升模型处理长文本的能力是当前的研究热点。

*   **位置编码的改进**:
    *   **ALiBi (Attention with Linear Biases)**: 放弃绝对位置编码，通过在注意力分数上添加一个与距离成正比的偏置项，使模型能更好地外推到未见过的长序列。
    *   **RoPE (Rotary Position Embedding)**: 通过旋转矩阵来对 Query 和 Key 进行编码，引入了相对位置信息，具有很好的外推性。被 Llama 等主流模型采用。
*   **窗口化注意力 (Windowed Attention)**: 如上文稀疏注意力所述，限制每个词只关注其邻近的词。

### 4. 多模态模型的架构融合

为了让模型能够理解和处理文本以外的信息（如图像、音频），研究人员探索了多种架构融合方案。

*   **跨模态编码器**: 使用一个独立的编码器（如 ViT 用于图像）将非文本信息编码成与文本表示空间对齐的向量序列。
*   **融合方式**:
    *   将图像等模态的向量序列拼接到文本序列前面，让一个统一的 Decoder-Only LLM 进行处理（如 LLaVA）。
    *   通过交叉注意力机制将不同模态的信息进行融合。
