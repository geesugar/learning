# 第 2 章：大模型架构

本章深入探讨现代大语言模型的核心架构——Transformer，并介绍其主流变体和最新的架构演进趋势。

## 学习内容

1.  [**Transformer 详解**](./01-Transformer详解.md)
    *   深入理解构成 Transformer 的每一个核心组件，包括自注意力、多头注意力、位置编码等。

2.  [**主流模型架构分析**](./02-主流模型架构分析.md)
    *   对比分析三种主流架构：Encoder-Only (如 BERT)、Decoder-Only (如 GPT) 和 Encoder-Decoder (如 T5)，了解它们各自的优缺点和适用场景。

3.  [**架构的演进与创新**](./03-架构的演进与创新.md)
    *   探索为了解决性能瓶颈和扩展模型能力而出现的前沿技术，如 FlashAttention、混合专家模型 (MoE) 和长上下文处理技术。