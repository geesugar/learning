# 2.2 主流模型架构分析

基于 Transformer 的核心组件，研究界衍生出了三种主流的模型架构。它们在结构和应用上各有侧重，适用于不同的任务。

### 1. Encoder-Only 架构 (编码器-仅)

这种架构只使用 Transformer 的 Encoder 部分。

*   **代表模型**: BERT, RoBERTa, DeBERTa
*   **工作原理**:
    *   通过自注意力机制，模型可以同时看到输入序列的“左侧上下文”和“右侧上下文”（即双向信息）。
    *   通常使用**掩码语言建模 (Masked Language Modeling, MLM)** 作为训练目标：随机遮盖输入文本中的一部分词 (token)，然后让模型预测这些被遮盖的词。
*   **优点**:
    *   对输入的上下文有深刻、双向的理解。
*   **缺点**:
    *   不适合生成式任务，因为其设计初衷是理解和表示，而非按顺序生成文本。
*   **典型应用**:
    *   文本分类、情感分析
    *   命名实体识别 (NER)
    *   句子关系判断 (如：问答、自然语言推断)
    *   作为特征提取器，为下游任务提供高质量的文本嵌入。

### 2. Decoder-Only 架构 (解码器-仅)

这种架构只使用 Transformer 的 Decoder 部分。这是当今绝大多数大语言模型（LLM）采用的架构。

*   **代表模型**: GPT 系列 (GPT-2, GPT-3, GPT-4), Llama, PaLM, Mistral
*   **工作原理**:
    *   在自注意力计算时，使用**掩码机制 (Masking)**，确保每个位置的词只能关注到它之前的词（即单向信息，从左到右）。这被称为**因果语言建模 (Causal Language Modeling, CLM)**。
    *   训练目标是预测序列中的下一个词。
*   **优点**:
    *   天然适用于文本生成任务，如续写、对话、翻译等。
    *   架构统一，易于扩展到极大的参数规模。
*   **缺点**:
    *   对于需要深度双向上下文理解的任务，理论上不如 Encoder-Only 模型。但当模型规模足够大时，这种差距会减小。
*   **典型应用**:
    *   对话系统 (Chatbots)
    *   内容创作、代码生成
    *   摘要、翻译
    *   几乎所有开放式的生成任务。

### 3. Encoder-Decoder 架构 (编码器-解码器)

这种架构同时使用 Transformer 的 Encoder 和 Decoder 两部分，也称为序列到序列 (Sequence-to-Sequence) 模型。

*   **代表模型**: T5, BART, 原始 Transformer
*   **工作原理**:
    *   Encoder 负责将完整的输入序列编码成一个中间表示。
    *   Decoder 在该中间表示的指导下，自回归地生成输出序列。Decoder 不仅会关注已生成的输出（通过自注意力），还会关注 Encoder 的全部输出（通过**交叉注意力 Cross-Attention**）。
*   **优点**:
    *   非常适合输入和输出序列结构不同的“转换”类任务。
*   **缺点**:
    *   结构相对复杂，参数量通常较大。
*   **典型应用**:
    *   机器翻译（最经典的应用）
    *   文本摘要
    *   问答系统（将问题作为输入，答案作为输出）
