# 4.3 优化器与训练稳定性

选择合适的优化器并确保训练过程的数值稳定性，是成功训练大模型的关键技术保障。

## 1. 优化器 (Optimizer)

优化器负责根据损失函数计算出的梯度来更新模型的参数（权重）。

*   **AdamW**: 
    *   **描述**: AdamW 是目前训练 Transformer 模型最常用、最鲁棒的优化器。它是对标准 Adam 算法的改进，主要区别在于处理权重衰减（Weight Decay）的方式。
    *   **Adam vs AdamW**: 在 Adam 中，权重衰减与 L2 正则化等价，并与梯度耦合在一起。而在 AdamW 中，权重衰减是直接从权重中减去的，与梯度更新解耦。实践证明，这种方式对于 Transformer 这类模型更有效。

*   **新型优化器**: 
    *   **Adafactor**: 由 Google 提出，主要用于节省显存。它不存储完整的优化器状态（如 Adam 的动量），而是通过滚动平均等方式来估算，但可能会牺牲一些性能。
    *   **Sophia**: 一种新的二阶优化器，声称在某些场景下比 AdamW 更快、更高效，但尚未成为主流选择。

## 2. 训练稳定性技术

大模型（尤其是超过百亿参数的）的训练过程非常容易出现数值不稳定问题，导致损失（Loss）突然飙升（spikes）甚至变成 NaN (Not a Number)，从而中断训练。

*   **混合精度训练 (Mixed-Precision Training)**:
    *   **原理**: 同时使用 16 位浮点数（FP16 或 BF16）和 32 位浮点数（FP32）进行训练。
        *   **FP16/BF16**: 用于存储模型权重、激活值和计算梯度，可以大幅减少显存占用，并利用现代 GPU 的 Tensor Cores 加速计算。
        *   **FP32**: 用于存储梯度的累积值和更新权重，以保持足够的精度，避免数值下溢。
    *   **动态损失缩放 (Dynamic Loss Scaling)**: 为了防止在使用 FP16 时梯度因数值太小而下溢（变为 0），在反向传播前，将损失值乘以一个缩放因子；在更新权重前，再将梯度除以该因子。缩放因子会动态调整，以适应梯度的范围。
    *   **BF16 vs FP16**: BFloat16 (BF16) 的动态范围与 FP32 相同，但精度较低，更不容易发生下溢，因此在现代硬件上（如 A100, H100）更受青睐。

*   **梯度裁剪 (Gradient Clipping)**:
    *   **描述**: 防止梯度爆炸的常用技术。它为梯度的范数（norm）设定一个上限阈值。如果在某次迭代中，梯度的范数超过了这个阈值，就将其缩放到阈值大小。
    *   **作用**: 这可以防止单次迭代中过大的梯度对模型权重造成破坏性更新，从而避免损失尖峰。

*   **权重初始化 (Weight Initialization)**:
    *   **描述**: 合理的权重初始化对训练初期的稳定性至关重要。通常采用特定的正态分布或均匀分布，使其标准差与模型的深度和宽度相关联，以保证信号在网络中能够顺畅地传播。
