# 4.1 训练目标与损失函数

预训练的目标是让模型在海量的无标签文本上学习通用的语言知识，如语法、事实知识、推理能力等。这个过程通过自监督学习（Self-supervised Learning）完成，即模型从数据自身中创造标签进行学习。主流的训练目标有两种：因果语言建模和掩码语言建模。

## 1. 因果语言建模 (Causal Language Modeling, CLM)

这是目前 Decoder-Only 架构（如 GPT 系列）LLM 的标准训练方式。

*   **目标**: 预测序列中的下一个词 (token)。
*   **过程**:
    1.  给定一个文本序列 `x = (x_1, x_2, ..., x_t)`。
    2.  模型在预测第 `i` 个词 `x_i` 时，只能看到它前面的所有词 `(x_1, ..., x_{i-1})`，而不能看到它后面的词。这通过在自注意力层使用掩码（Causal Mask）来实现。
    3.  模型输出一个在整个词汇表上的概率分布，表示下一个词的可能性。
*   **损失函数**: 通常使用**交叉熵损失 (Cross-Entropy Loss)**。它衡量的是模型预测的概率分布与真实下一个词（one-hot 编码）之间的差异。
    *   对于整个序列，损失是所有位置预测损失的平均值或总和。
*   **优点**: 天然适合文本生成任务，因为其训练方式和生成方式完全一致。
*   **架构**: 主要用于 Decoder-Only 模型。

## 2. 掩码语言建模 (Masked Language Modeling, MLM)

这是 Encoder-Only 架构（如 BERT）的经典训练方式。

*   **目标**: 预测输入序列中被随机遮盖（mask）掉的词。
*   **过程**:
    1.  从输入序列中随机选择 15% 的词。
    2.  对于被选中的词：
        *   80% 的概率，用一个特殊的 `[MASK]` 标记替换它。
        *   10% 的概率，用一个随机的词替换它。
        *   10% 的概率，保持原词不变。
    3.  模型需要利用完整的、双向的上下文（即被遮盖词的左边和右边所有词）来预测这些位置的原始词汇。
*   **损失函数**: 只计算被遮盖位置的交叉熵损失。
*   **优点**: 能够学习到深刻的双向上下文表示，非常适合做文本理解和特征提取任务。
*   **缺点**: 
    *   训练和推理之间存在不匹配（`[MASK]` 标记在推理时不存在）。
    *   每次只预测 15% 的词，相比 CLM，其训练效率（sample efficiency）较低。

## 3. 混合方法 (Denoising Objectives)

Encoder-Decoder 架构（如 T5, BART）采用去噪自编码器（Denoising Autoencoder）的目标，可以看作是 MLM 的一种泛化。

*   **BART 的方法**: 破坏原始文本（如删除、遮盖、打乱语序），然后让模型完整地恢复出原始文本。
*   **T5 的方法**: 将文本中连续的一段（span）替换为单个掩码标记（如 `<X>`），然后让模型在解码端生成被替换掉的完整文本段。这种 “span corruption” 的方式更具挑战性。
