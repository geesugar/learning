# 1.1 数学基础

深刻理解大语言模型背后的数学原理，是进行模型创新、问题诊断和性能优化的基础。以下是三个最关键的数学领域。

## 1. 线性代数 (Linear Algebra)

线性代数是描述和操作高维数据的语言。在 LLM 中，无论是词嵌入、模型权重还是中间层的激活值，都是以向量、矩阵或张量的形式存在的。

*   **向量 (Vectors)**: 
    *   **定义**: 一维数组，表示空间中的一个点或一个方向。
    *   **应用**: 词嵌入（Word Embedding）就是将一个词表示为一个高维向量。
*   **矩阵 (Matrices)**: 
    *   **定义**: 二维数组，可以看作是多个向量的组合。
    *   **应用**: 模型的权重（例如，线性层的权重 `W`）通常是一个矩阵。输入数据（一批句子的嵌入）也可以表示为一个矩阵。
*   **张量 (Tensors)**: 
    *   **定义**: 矩阵向更高维度的推广。向量是一阶张量，矩阵是二阶张量。
    *   **应用**: 深度学习框架（如 PyTorch, TensorFlow）中的基本数据结构就是张量，可以处理任意维度的数据（如批次、序列长度、嵌入维度）。
*   **核心运算**:
    *   **矩阵乘法**: 神经网络中线性变换的核心，是计算量最大的部分。
    *   **点积 (Dot Product)**: 注意力机制中计算查询（Query）和键（Key）之间相似度的基础。

## 2. 微积分 (Calculus)

微积分，特别是微分，是模型学习（即优化）过程的数学基石。

*   **导数 (Derivatives)**: 
    *   **定义**: 衡量函数在某一点的瞬时变化率。
    *   **应用**: 导数告诉我们，当某个模型参数（如一个权重）发生微小变化时，损失函数会如何变化。
*   **梯度 (Gradients)**: 
    *   **定义**: 多变量函数（如损失函数）的导数，是一个向量，指向函数值上升最快的方向。
    *   **应用**: **梯度下降法**的核心。模型沿着梯度的**相反方向**更新参数，从而使损失函数最小化。
*   **链式法则 (Chain Rule)**: 
    *   **定义**: 用于计算复合函数的导数。
    *   **应用**: **反向传播 (Backpropagation)** 算法的数学基础。它使得我们能够高效地计算出损失函数相对于模型中每一层、每一个参数的梯度，无论网络有多深。

## 3. 概率论与统计 (Probability & Statistics)

概率论为处理不确定性、生成文本和评估模型提供了理论框架。

*   **概率分布 (Probability Distributions)**: 
    *   **定义**: 描述一个随机变量所有可能取值及其对应概率的函数。
    *   **应用**: LLM 的输出层（Softmax）实际上是在词汇表上生成一个概率分布，表示下一个词是每个候选词的概率。
*   **贝叶斯定理 (Bayes' Theorem)**: 
    *   **定义**: 描述了在给定新证据的情况下，如何更新一个事件的概率。
    *   **应用**: 是许多概率建模思想的基础，帮助理解模型如何根据上下文更新其预测。
*   **信息论 (Information Theory)**:
    *   **交叉熵 (Cross-Entropy)**: 衡量两个概率分布之间的差异。在 LLM 训练中，它被用作**损失函数**，衡量模型预测的词汇概率分布与真实词汇（one-hot 分布）之间的差距。
    *   **困惑度 (Perplexity)**: 交叉熵损失的指数形式，是评估语言模型性能的常用指标，数值越低表示模型性能越好。
