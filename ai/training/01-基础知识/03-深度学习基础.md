# 1.3 深度学习基础

深度学习是利用深度神经网络（具有多个隐藏层）进行学习的机器学习分支。LLM 正是深度学习在自然语言处理领域最杰出的成果。

## 1. 神经网络 (Neural Networks)

*   **基本单元：神经元 (Neuron)**: 
    *   接收一组输入，通过一个线性变换（加权求和），然后应用一个非线性**激活函数 (Activation Function)** 得到输出。
    *   **激活函数 (如 ReLU, GeLU)**: 为模型引入非线性，使其能够学习比简单线性关系更复杂的模式。没有非线性激活函数的深层网络，本质上等同于一个单层线性模型。

*   **网络结构：层 (Layers)**:
    *   神经元被组织成层。一个简单的全连接层（或线性层）由多个神经元组成，每个神经元都与前一层的所有输出相连。
    *   **深度神经网络**: 通过堆叠多个层（隐藏层）来构建。网络的“深度”使其能够学习到从低级特征（如边缘）到高级概念（如对象）的层次化表示。

*   **反向传播 (Backpropagation)**:
    *   训练神经网络的核心算法。它利用微积分中的**链式法则**，从输出层的损失开始，逐层向后计算损失函数相对于网络中每个参数的梯度。这些梯度随后被优化器用来更新网络权重。

## 2. 处理序列数据的经典模型

在 Transformer 出现之前，处理序列数据（如文本）主要依赖于循环神经网络。

*   **循环神经网络 (Recurrent Neural Networks, RNN)**:
    *   **核心思想**: 具有“记忆”能力。在处理序列中的当前元素时，会利用一个隐藏状态（Hidden State）来编码之前所有元素的信息。
    *   **缺点**: 存在**梯度消失/爆炸**问题，导致其难以学习到长距离的依赖关系（例如，一个段落开头和结尾的词之间的关系）。

*   **长短期记忆网络 (Long Short-Term Memory, LSTM)**:
    *   **核心思想**: RNN 的一种复杂变体，专门设计用来解决长距离依赖问题。它引入了**门控机制 (Gating Mechanism)**——输入门、遗忘门和输出门，这些门可以控制信息在隐藏状态中的流动，决定哪些信息被保留、哪些被遗忘。
    *   **影响**: 在 Transformer 出现前，LSTM 是 NLP 领域的绝对主力。

## 3. 注意力机制 (Attention Mechanism)

注意力机制是深度学习领域的一大突破，也是 Transformer 架构的基石。

*   **核心思想**: 模仿人类的注意力。在处理一个序列时，不是将整个序列压缩成一个固定的上下文向量，而是允许模型在生成输出的每一步，动态地、有选择地关注输入序列中不同部分的信息。
*   **演进**: 最初被用于改进 RNN/LSTM 在机器翻译等任务上的表现。模型在解码（生成译文）的每一步，都会“回头看”并重点关注输入句子中与当前要生成的词最相关的几个词。
*   **自注意力 (Self-Attention)**: Transformer 将这一思想推向极致。它完全抛弃了 RNN 的循环结构，让输入序列的每个元素都与其他所有元素计算注意力权重。这使得模型能够一步到位地捕捉全局依赖关系，并且极大地利于并行计算。
