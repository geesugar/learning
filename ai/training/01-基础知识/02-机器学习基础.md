# 1.2 机器学习基础

大语言模型是机器学习，特别是深度学习领域的一个分支。理解核心的机器学习概念对于掌握 LLM 的训练和应用至关重要。

## 1. 学习范式 (Learning Paradigms)

*   **监督学习 (Supervised Learning)**:
    *   **定义**: 使用带有明确标签（label）的数据进行训练。模型学习从输入到输出的映射关系。
    *   **LLM 中的应用**: **指令微调 (SFT)** 是一个典型的监督学习过程。输入是指令，输出是期望的回答，模型学习去复现这种“正确答案”。

*   **无监督学习 (Unsupervised Learning)**:
    *   **定义**: 使用没有标签的数据进行训练。模型学习数据本身的内在结构、模式或分布。
    *   **LLM 中的应用**: 传统的聚类、降维等任务。在 LLM 语境下，其概念常与自监督学习交叉。

*   **自监督学习 (Self-supervised Learning)**:
    *   **定义**: 无监督学习的一种特殊形式，也是现代 LLM 的基石。它从无标签的数据中**自动创建标签**，从而将问题转化为监督学习的形式。
    *   **LLM 中的应用**: **预训练 (Pre-training)** 过程就是自监督学习。例如，在因果语言建模（CLM）中，输入是句子的前 `n-1` 个词，标签就是第 `n` 个词，这个标签是从数据自身中获取的，无需人工标注。

## 2. 核心组件

*   **损失函数 (Loss Function)**:
    *   **定义**: 一个衡量模型预测值与真实标签之间差距的函数。它是模型优化的目标，即最小化损失函数的值。
    *   **LLM 中的应用**: 最常用的是**交叉熵损失 (Cross-Entropy Loss)**，用于评估模型预测的下一个词的概率分布与真实情况的差异。

*   **优化算法 (Optimization Algorithm)**:
    *   **定义**: 根据损失函数计算出的梯度来更新模型参数，以期降低损失的算法。
    *   **梯度下降 (Gradient Descent)**: 最基础的优化算法。沿着损失函数梯度的反方向更新参数。
    *   **Adam / AdamW**: 目前 LLM 训练中最主流的优化器。它结合了动量（Momentum）和自适应学习率的优点，能够快速、稳定地进行优化。

## 3. 训练中的关键问题

*   **过拟合 (Overfitting)**:
    *   **定义**: 模型在训练数据上表现很好，但在未见过的测试数据上表现很差。这说明模型“记住”了训练数据的噪声和细节，而不是学习到底层的通用规律。
    *   **LLM 中的对策**: 使用海量的预训练数据、正则化技术（如 Dropout, Weight Decay）、早停（Early Stopping）等。

*   **欠拟合 (Underfitting)**:
    *   **定义**: 模型在训练数据上表现就很差，说明模型容量不足，无法捕捉数据中的复杂模式。
    *   **LLM 中的对策**: 增加模型的大小（层数、宽度）、延长训练时间、使用更强大的模型架构。

*   **正则化 (Regularization)**:
    *   **定义**: 一系列旨在防止过拟合、提升模型泛化能力的技术。
    *   **常见方法**: 
        *   **权重衰减 (Weight Decay)**: 在损失函数中加入一个惩罚项，限制模型权重的大小，防止其变得过大。
        *   **Dropout**: 在训练过程中，以一定的概率随机地“丢弃”（即置零）一部分神经元的输出。这强迫网络学习更鲁棒的特征，而不是依赖于少数几个神经元。
