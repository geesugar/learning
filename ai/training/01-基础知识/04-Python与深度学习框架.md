# 1.4 Python 与深度学习框架

将理论付诸实践，离不开强大的编程语言和高效的深度学习框架。Python 凭借其简洁的语法和庞大的生态系统，已成为 AI 领域的通用语言。

## 1. Python

*   **为什么是 Python**: 
    *   **易学易用**: 语法清晰，上手快，让研究人员可以专注于算法和模型设计。
    *   **强大的科学计算库**: NumPy, SciPy, Pandas 等库为数据处理和数值计算提供了坚实的基础。
    *   **胶水语言特性**: 可以轻松地与 C++/CUDA 等高性能代码库集成，兼顾了开发效率和运行效率。

## 2. 主流深度学习框架

深度学习框架提供了构建、训练和部署神经网络所需的核心工具，包括自动求导、GPU 加速、预置的神经网络层和优化器等。

*   **PyTorch (Meta AI)**:
    *   **特点**: 
        *   **灵活性和易用性**: 以其“Pythonic”的接口和动态计算图（Define-by-Run）而闻名，使得调试和模型设计非常直观和灵活。
        *   **学术界首选**: 在研究社区中拥有极高的占有率，最新的研究成果通常会先有 PyTorch 实现。
        *   **强大的生态**: 拥有丰富的库和工具，如 `torch.distributed` (用于分布式训练), `torch.fx` (用于模型变换) 等。
    *   **结论**: 对于学习和研究 LLM，PyTorch 是目前事实上的标准。

*   **TensorFlow (Google)**:
    *   **特点**: 
        *   **工业级部署**: 以其强大的生产部署能力（TensorFlow Serving）和跨平台支持（TensorFlow Lite）而著称。
        *   **静态计算图**: 传统上使用静态计算图（Define-and-Run），虽然在灵活性上稍逊，但有利于进行图优化和部署。
        *   **Keras**: TensorFlow 的高级 API，极大地简化了模型构建过程。
    *   **结论**: 在工业界，尤其是在需要复杂部署场景时，仍有广泛应用。

## 3. Hugging Face 生态系统

Hugging Face 已经成为 NLP 和 LLM 领域不可或缺的平台和工具集，极大地降低了使用和训练先进模型的门槛。

*   **`transformers`**: 
    *   **核心库**: 提供了数千个预训练模型的实现（包括 GPT, Llama, BERT, T5 等）和统一的调用接口。你可以用几行代码就加载并使用一个强大的预训练模型。
    *   **模型中心 (Model Hub)**: 一个庞大的模型仓库，用户可以轻松下载和分享模型权重。

*   **`datasets`**: 
    *   **功能**: 提供对数千个常用数据集的访问，并提供了高效的数据处理和加载工具。

*   **`tokenizers`**: 
    *   **功能**: 提供了多种主流 Tokenization 算法（BPE, WordPiece）的高性能实现。

*   **`accelerate`**: 
    *   **功能**: 极大地简化了在不同硬件（单 GPU, 多 GPU, TPU）上进行分布式训练的代码编写，屏蔽了底层的复杂性。

*   **`peft`**: 
    *   **功能**: 提供了对 LoRA, Prompt Tuning 等多种参数高效微调（PEFT）方法的简洁实现。

**总结**: 对于任何想要训练或微调 LLM 的人来说，熟练掌握 Python、PyTorch 以及 Hugging Face 的核心库 (`transformers`, `datasets`, `accelerate`, `peft`) 是必备的技能。
