# DeepSeek 模型系列

DeepSeek（深度求索）是由中国初创公司深度求索智能科技推出的一系列大型语言模型，以其卓越的开源模型和极具竞争力的能力表现受到广泛关注。

## 发展历史

### 创立背景
- **2023年4月**：深度求索智能科技由前字节跳动AI实验室研究员杨植麟创立
- **技术路线**：聚焦构建强大且开放的基础模型，涵盖通用语言理解、代码生成、数学推理等能力
- **开源策略**：采用更开放的许可证发布高质量基础模型，促进AI研究和应用发展

### 模型演进

- **DeepSeek-LLM 7B/67B** (2023年11月)：首批发布的基础语言模型
- **DeepSeek-Coder 6.7B/33B** (2023年11月)：专注于代码生成的专业模型
- **DeepSeek-MoE 16B** (2024年1月)：混合专家模型，平衡效率与能力
- **DeepSeek-Math 7B** (2024年3月)：专注于数学推理能力的模型
- **DeepSeek-VL** (2024年4月)：视觉语言多模态模型
- **DeepSeek-R1/DeepSeek-R1-Zero** (2024年12月)：基于DeepSeek-V3的推理增强大模型，使用强化学习技术提升推理能力
- **DeepSeek-R1-Distill系列** (2025年1月)：将DeepSeek-R1的推理能力蒸馏到更小规模模型的系列，包括基于Llama和Qwen的多个版本

## 核心技术与架构

DeepSeek模型系列采用了一系列创新的架构设计和训练方法，下面详细介绍各主要模型版本的架构特点。

### DeepSeek-LLM 架构

DeepSeek-LLM是一个基于Transformer架构的自回归语言模型，有7B和67B两种参数规模。

```
┌─────────────────────────────────────────┐
│        DeepSeek-LLM 架构图              │
├─────────────────────────────────────────┤
│                                         │
│           ┌──────────────┐              │
│           │   输出层     │              │
│           └──────────────┘              │
│                   ▲                     │
│                   │                     │
│       ┌───────────────────────┐         │
│       │ N x Transformer Block │         │
│       └───────────────────────┘         │
│                   ▲                     │
│                   │                     │
│           ┌──────────────┐              │
│           │  Token嵌入层  │              │
│           └──────────────┘              │
│                   ▲                     │
│                   │                     │
│           ┌──────────────┐              │
│           │    输入ID     │              │
│           └──────────────┘              │
│                                         │
└─────────────────────────────────────────┘

● 模型层数对比:
  - DeepSeek-LLM-7B:  32层Transformer块
  - DeepSeek-LLM-67B: 70层Transformer块
```

**架构特点**:
- **RoPE位置编码**：采用旋转位置编码，支持更长的上下文推理
- **SwiGLU激活函数**：替代传统ReLU，提高非线性表达能力
- **Group Query Attention**：优化注意力机制，提高训练和推理效率
- **扩展上下文窗口**：原生支持8K，通过延长位置插值支持高达128K

### DeepSeek-Coder 架构

DeepSeek-Coder是专门为代码生成优化的模型，基于DeepSeek-LLM架构但针对代码语料和任务进行了专门优化。

```
┌──────────────────────────────────────────┐
│          DeepSeek-Coder 架构图           │
├──────────────────────────────────────────┤
│                                          │
│           ┌───────────────┐              │
│           │    输出层     │              │
│           └───────────────┘              │
│                   ▲                      │
│                   │                      │
│       ┌────────────────────────┐         │
│       │ N x Transformer Block  │         │
│       │ (代码优化的注意力头)    │         │
│       └────────────────────────┘         │
│                   ▲                      │
│                   │                      │
│           ┌───────────────┐              │
│           │   填充适配层   │              │
│           └───────────────┘              │
│                   ▲                      │
│                   │                      │
│           ┌───────────────┐              │
│           │ 代码Token嵌入层 │              │
│           └───────────────┘              │
│                   ▲                      │
│                   │                      │
│           ┌───────────────┐              │
│           │   代码输入ID   │              │
│           └───────────────┘              │
│                                          │
└──────────────────────────────────────────┘

● 模型层数对比:
  - DeepSeek-Coder-6.7B: 32层Transformer块
  - DeepSeek-Coder-33B:  60层Transformer块
```

**架构特点**:
- **代码特化的Tokenizer**：针对多种编程语言优化的分词器
- **Fill-in-the-Middle训练**：支持代码补全和编辑能力
- **多编程语言预训练**：在超过2万亿tokens的代码数据上训练
- **低温采样优化**：针对代码生成进行特殊优化，提高确定性

### DeepSeek-MoE 架构

DeepSeek-MoE采用稀疏混合专家(Mixture of Experts)架构，通过激活部分专家网络来处理不同输入，大幅提高参数效率。

```
┌───────────────────────────────────────────────┐
│            DeepSeek-MoE 架构图                │
├───────────────────────────────────────────────┤
│                                               │
│              ┌───────────────┐                │
│              │    输出层     │                │
│              └───────────────┘                │
│                      ▲                        │
│                      │                        │
│      ┌────────────────────────────────┐       │
│      │        N x MoE Block           │       │
│      │  ┌─────────┐     ┌─────────┐   │       │
│      │  │ 路由器   │────>│ 专家网络 │   │       │
│      │  └─────────┘  │  └─────────┘   │       │
│      │               │                │       │
│      │               │  ┌─────────┐   │       │
│      │               └─>│ 专家网络 │   │       │
│      │                  └─────────┘   │       │
│      └────────────────────────────────┘       │
│                      ▲                        │
│                      │                        │
│              ┌───────────────┐                │
│              │ Token嵌入层   │                │
│              └───────────────┘                │
│                      ▲                        │
│                      │                        │
│              ┌───────────────┐                │
│              │    输入ID     │                │
│              └───────────────┘                │
│                                               │
└───────────────────────────────────────────────┘

● DeepSeek-MoE-16B 结构:
  - 32层Transformer块
  - 每层8个专家网络
  - 激活顶部2个专家
  - 实际参数量: 16B
  - 有效参数量: ~70B
```

**架构特点**:
- **稀疏激活机制**：每次只激活一部分专家网络处理输入
- **路由器网络**：学习将不同输入分配给最适合的专家
- **专家平衡损失**：训练时确保各专家负载均衡
- **辅助损失**：通过额外损失函数优化路由决策

### DeepSeek-Math 架构

DeepSeek-Math专为数学推理设计，在基础模型上增加了特殊的数学形式化表示处理能力。

```
┌────────────────────────────────────────────┐
│          DeepSeek-Math 架构图              │
├────────────────────────────────────────────┤
│                                            │
│            ┌───────────────┐               │
│            │    输出层     │               │
│            └───────────────┘               │
│                    ▲                       │
│                    │                       │
│        ┌───────────────────────┐           │
│        │ N x Transformer Block │           │
│        └───────────────────────┘           │
│                    ▲                       │
│                    │                       │
│            ┌───────────────┐               │
│            │ 数学符号处理层 │               │
│            └───────────────┘               │
│                    ▲                       │
│                    │                       │
│            ┌───────────────┐               │
│            │  Token嵌入层  │               │
│            └───────────────┘               │
│                    ▲                       │
│                    │                       │
│            ┌───────────────┐               │
│            │    输入ID     │               │
│            └───────────────┘               │
│                                            │
└────────────────────────────────────────────┘

● DeepSeek-Math-7B:
  - 32层Transformer块
  - 数学特化的训练策略
  - 形式化推理增强
```

**架构特点**:
- **数学公式表示优化**：特殊处理LaTeX、MathML等数学表示
- **思维链训练**：通过步骤分解提高推理能力
- **形式验证训练**：学习验证推导步骤的正确性
- **推理轨迹优化**：特殊训练以提高中间步骤生成质量

### DeepSeek-VL 架构

DeepSeek-VL是一个多模态模型，结合了视觉编码器和大语言模型，能够理解和生成关于图像的文本。

```
┌────────────────────────────────────────────────┐
│               DeepSeek-VL 架构图               │
├────────────────────────────────────────────────┤
│                                                │
│               ┌───────────────┐                │
│               │    输出层     │                │
│               └───────────────┘                │
│                       ▲                        │
│                       │                        │
│           ┌───────────────────────┐            │
│           │ N x Transformer Block │            │
│           └───────────────────────┘            │
│                       ▲                        │
│                       │                        │
│               ┌───────────────┐                │
│               │  多模态融合层  │                │
│               └───────────────┘                │
│               ▲             ▲                  │
│               │             │                  │
│    ┌───────────────┐  ┌───────────────┐        │
│    │  视觉编码器   │  │  Token嵌入层  │        │
│    └───────────────┘  └───────────────┘        │
│            ▲                  ▲                │
│            │                  │                │
│    ┌───────────────┐  ┌───────────────┐        │
│    │    图像输入   │  │    文本输入   │        │
│    └───────────────┘  └───────────────┘        │
│                                                │
└────────────────────────────────────────────────┘

● DeepSeek-VL 结构:
  - 视觉编码器: 基于修改的CLIP/ViT架构
  - 语言模型: 基于DeepSeek-LLM架构
  - 多模态映射层: 将视觉特征映射到语言空间
```

**架构特点**:
- **视觉编码器**：基于改进的Vision Transformer架构
- **跨模态投影**：专门设计的投影层将视觉特征映射到语言空间
- **对比学习**：使用图像-文本对进行对比学习
- **视觉指令微调**：通过大量视觉指令数据进行微调以提高理解能力

### DeepSeek-R1 架构

DeepSeek-R1是一个专注于强化推理能力的大型模型，基于DeepSeek-V3-Base模型进行强化学习训练，采用MoE（混合专家）架构。

```
┌────────────────────────────────────────────────┐
│            DeepSeek-R1 架构图                   │
├────────────────────────────────────────────────┤
│                                                │
│               ┌───────────────┐                │
│               │    输出层     │                │
│               └───────────────┘                │
│                      ▲                         │
│                      │                         │
│     ┌─────────────────────────────────┐        │
│     │       强化学习优化的MoE层        │        │
│     │   (多个专家网络和路由器组件)     │        │
│     └─────────────────────────────────┘        │
│                      ▲                         │
│                      │                         │
│               ┌───────────────┐                │
│               │ 思维链生成层  │                │
│               └───────────────┘                │
│                      ▲                         │
│                      │                         │
│               ┌───────────────┐                │
│               │ Token嵌入层   │                │
│               └───────────────┘                │
│                      ▲                         │
│                      │                         │
│               ┌───────────────┐                │
│               │    输入ID     │                │
│               └───────────────┘                │
│                                                │
└────────────────────────────────────────────────┘

● DeepSeek-R1结构:
  - 基于DeepSeek-V3-Base
  - 总参数量: 671B
  - 激活参数量: 37B
  - 上下文窗口: 128K
```

**架构特点**:
- **强化学习训练**：不依赖传统的监督微调，直接使用强化学习提升推理能力
- **思维链(Chain-of-Thought)机制**：自然涌现出分解复杂问题和逐步推理的能力
- **多阶段训练流程**：包括两个强化学习阶段和两个监督微调阶段
- **自我验证能力**：能够检查和反思自己的推理步骤和结论

### DeepSeek-R1-Distill 系列

DeepSeek-R1-Distill系列是将DeepSeek-R1的强大推理能力蒸馏到规模更小的模型中的产品线，基于主流开源模型架构如Llama和Qwen进行开发。

```
┌────────────────────────────────────────────────────┐
│           DeepSeek-R1-Distill 架构图              │
├────────────────────────────────────────────────────┤
│                                                    │
│ ┌────────────────────┐    ┌────────────────────┐   │
│ │  DeepSeek-R1 671B  │    │  目标模型架构      │   │
│ │  (教师模型)        │    │  (学生模型)        │   │
│ └────────────────────┘    └────────────────────┘   │
│          │                           ▲             │
│          │                           │             │
│          │      ┌─────────────────┐  │             │
│          └─────>│ 知识蒸馏过程    │──┘             │
│                 └─────────────────┘                │
│                                                    │
└────────────────────────────────────────────────────┘

● DeepSeek-R1-Distill系列模型:
  - DeepSeek-R1-Distill-Qwen-1.5B (基于Qwen2.5-Math-1.5B)
  - DeepSeek-R1-Distill-Qwen-7B (基于Qwen2.5-Math-7B)
  - DeepSeek-R1-Distill-Llama-8B (基于Llama-3.1-8B)
  - DeepSeek-R1-Distill-Qwen-14B (基于Qwen2.5-14B)
  - DeepSeek-R1-Distill-Qwen-32B (基于Qwen2.5-32B)
  - DeepSeek-R1-Distill-Llama-70B (基于Llama-3.3-70B-Instruct)
```

**架构特点**:
- **借用现有成熟架构**：基于广泛使用的Llama和Qwen等模型架构，确保良好的生态系统兼容性
- **知识蒸馏方法**：使用DeepSeek-R1生成的推理样本训练小模型模仿大模型的能力
- **保留底层架构优势**：继承基础模型的高效推理和部署便利性
- **微调配置和分词器**：对基础模型进行适当调整以优化蒸馏效果
- **保留许可证友好性**：遵循MIT许可证，同时尊重原始模型的许可要求

## 模型参数规模与性能对比

下面的表格比较了DeepSeek各模型版本的主要参数和性能特点：

| 模型名称 | 参数规模 | 上下文窗口 | 训练数据量 | 主要优势 | 典型应用场景 |
|---------|---------|-----------|-----------|---------|-------------|
| DeepSeek-LLM-7B | 7B | 8K-128K | 2T tokens | 通用能力均衡 | 内容生成、对话 |
| DeepSeek-LLM-67B | 67B | 8K-128K | 2T+ tokens | 强大推理能力 | 复杂推理、创意写作 |
| DeepSeek-Coder-6.7B | 6.7B | 16K | 2T 代码tokens | 代码生成高效 | 中小型编程任务 |
| DeepSeek-Coder-33B | 33B | 16K | 2T+ 代码tokens | 复杂代码生成 | 大型软件开发 |
| DeepSeek-MoE-16B | 16B(70B)* | 8K | 1.5T tokens | 参数效率高 | 资源受限场景 |
| DeepSeek-Math-7B | 7B | 8K | 900B+ tokens | 数学推理精确 | 数学教育、科研 |
| DeepSeek-VL | 7B | 4K | 视觉+语言数据 | 多模态理解 | 图像描述、视觉问答 |
| DeepSeek-R1 | 671B(37B)** | 128K | 特殊强化学习数据 | 强大推理能力 | 数学问题、代码生成、复杂推理 |
| DeepSeek-R1-Distill-Qwen-1.5B | 1.5B | 32K | R1蒸馏数据 | 轻量级推理 | 移动设备、边缘计算 |
| DeepSeek-R1-Distill-Qwen-7B | 7B | 32K | R1蒸馏数据 | 平衡性能和效率 | 通用推理任务 |
| DeepSeek-R1-Distill-Llama-8B | 8B | 32K | R1蒸馏数据 | 代码和数学能力 | 编程辅助、教育工具 |
| DeepSeek-R1-Distill-Qwen-14B | 14B | 32K | R1蒸馏数据 | 更强的推理能力 | 复杂问题求解 |
| DeepSeek-R1-Distill-Qwen-32B | 32B | 32K | R1蒸馏数据 | 接近大模型能力 | 企业级推理应用 |
| DeepSeek-R1-Distill-Llama-70B | 70B | 32K | R1蒸馏数据 | 最强蒸馏模型 | 高端推理服务 |

*MoE模型的有效参数量大于实际参数量
**MoE架构总参数量和激活参数量

## 模型能力基准测试

DeepSeek系列模型在多项基准测试中表现优异，以下是主要模型在关键基准上的表现：

### DeepSeek-LLM 基准测试

```
┌──────────────────────────────────────────────────────┐
│ DeepSeek-LLM 在关键基准上的性能 (分数百分比)           │
├──────────┬─────────┬────────┬────────┬───────┬───────┤
│ 模型      │ MMLU    │ HumanEval │ GSM8K │ MATH  │ C-Eval │
├──────────┼─────────┼────────┼────────┼───────┼───────┤
│ 7B       │ 63.0%   │ 35.4%  │ 61.7%  │ 24.5% │ 67.1% │
│ 67B      │ 78.5%   │ 72.6%  │ 88.3%  │ 51.9% │ 78.2% │
└──────────┴─────────┴────────┴────────┴───────┴───────┘
```

### DeepSeek-Coder 基准测试

```
┌───────────────────────────────────────────────────────┐
│ DeepSeek-Coder 在代码基准上的性能 (Pass@1 百分比)       │
├──────────┬───────────┬────────────┬─────────┬─────────┤
│ 模型      │ HumanEval │ MBPP       │ DS-1000 │ CodeContest │
├──────────┼───────────┼────────────┼─────────┼─────────┤
│ 6.7B     │ 67.1%     │ 69.8%      │ 75.3%   │ 34.2%   │
│ 33B      │ 82.3%     │ 81.2%      │ 84.7%   │ 45.6%   │
└──────────┴───────────┴────────────┴─────────┴─────────┘
```

### DeepSeek-Math 在数学能力基准测试中的表现

```
┌────────────────────────────────────────────────────┐
│ DeepSeek-Math 在数学基准上的性能 (准确率百分比)      │
├──────────┬─────────┬────────┬────────┬────────────┤
│ 模型      │ GSM8K   │ MATH   │ MMLU-Math │ MathContest │
├──────────┼─────────┼────────┼────────┬────────────┤
│ 7B       │ 89.7%   │ 52.8%  │ 65.3%  │ 30.2%      │
└──────────┴─────────┴────────┴────────┴────────────┘
```

### DeepSeek-R1-Distill系列基准测试

```
┌──────────────────────────────────────────────────────────────────────────┐
│      DeepSeek-R1-Distill系列在关键基准上的性能对比                        │
├──────────────────────┬───────────┬───────────┬────────────┬──────────────┤
│ 模型                  │ AIME 2024 │ MATH-500  │ GPQA Diamond│ CodeForces   │
│                      │ (pass@1)  │ (pass@1)  │ (pass@1)   │ (rating)     │
├──────────────────────┼───────────┼───────────┼────────────┼──────────────┤
│ GPT-4o               │ 9.3%      │ 74.6%     │ 49.9%      │ 759          │
│ Claude-3.5-Sonnet    │ 16.0%     │ 78.3%     │ 65.0%      │ 717          │
│ o1-mini              │ 63.6%     │ 90.0%     │ 60.0%      │ 1820         │
├──────────────────────┼───────────┼───────────┼────────────┼──────────────┤
│ Qwen-1.5B (蒸馏版)    │ 28.9%     │ 83.9%     │ 33.8%      │ 954          │
│ Qwen-7B (蒸馏版)      │ 55.5%     │ 92.8%     │ 49.1%      │ 1189         │
│ Llama-8B (蒸馏版)     │ 50.4%     │ 89.1%     │ 49.0%      │ 1205         │
│ Qwen-14B (蒸馏版)     │ 69.7%     │ 93.9%     │ 59.1%      │ 1481         │
│ Qwen-32B (蒸馏版)     │ 72.6%     │ 94.3%     │ 62.1%      │ 1691         │
│ Llama-70B (蒸馏版)    │ 70.0%     │ 94.5%     │ 65.2%      │ 1633         │
└──────────────────────┴───────────┴───────────┴────────────┴──────────────┘
```

## 使用方法

### 本地部署

DeepSeek模型大多以开源形式发布，可以通过Hugging Face Transformers库加载使用：

```python
# 加载DeepSeek-LLM模型示例
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载模型和分词器
model_name = "deepseek-ai/deepseek-llm-7b-base"  # 或其他DeepSeek模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 生成文本
inputs = tokenizer("请解释量子物理的基本原理", return_tensors="pt")
outputs = model.generate(**inputs, max_length=500)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### API使用

DeepSeek也提供API服务，可以通过HTTP请求调用：

```python
import requests
import json

API_URL = "https://api.deepseek.ai/v1/chat/completions"
API_KEY = "your_api_key"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

data = {
    "model": "deepseek-chat",  
    "messages": [
        {"role": "user", "content": "请解释量子物理的基本原理"}
    ],
    "temperature": 0.7,
    "max_tokens": 800
}

response = requests.post(API_URL, headers=headers, data=json.dumps(data))
result = response.json()
print(result["choices"][0]["message"]["content"])
```

### DeepSeek-R1-Distill模型使用

DeepSeek-R1-Distill系列模型可以通过Hugging Face Transformers库直接使用：

```python
# 加载DeepSeek-R1-Distill-Llama模型示例
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载模型和分词器
model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"  # 或其他DeepSeek-R1-Distill模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 设置推理参数（推荐温度设置在0.5-0.7之间）
inputs = tokenizer("求解方程：3x^2 + 5x - 2 = 0，请推导详细步骤并将最终答案放在\\boxed{}中。", return_tensors="pt")
outputs = model.generate(**inputs, max_length=2048, temperature=0.6)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

也可以通过vLLM等高效推理框架部署：

```bash
# 使用vLLM部署DeepSeek-R1-Distill-Qwen-32B模型
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768
```

注意：使用DeepSeek-R1系列模型时，建议遵循以下配置以达到预期性能：
1. 温度设置在0.5-0.7之间（推荐0.6）
2. 避免添加系统提示词，所有指令应包含在用户提示中
3. 对于数学问题，建议在提示中包含指令如："请逐步推理，并将最终答案放在\\boxed{}中"
4. 强制模型以"<think>\\n"开始回复，以确保模型进行充分推理

## 模型优势与局限性

### 优势

1. **开源友好**：采用更开放的许可证，便于研究和商业应用
2. **专业化模型**：针对代码、数学等领域的专门优化
3. **中英双语能力**：在中文和英文上都有不错的表现
4. **高效推理**：优化的架构设计使推理效率更高
5. **快速迭代**：团队持续改进模型能力和性能
6. **知识蒸馏策略**：通过蒸馏技术将大模型能力迁移至小模型
7. **开源模型架构集成**：基于主流开源架构开发，便于集成和部署

### 局限性

1. **通用知识广度**：相比某些闭源商业模型，通用知识覆盖还有提升空间
2. **幻觉倾向**：在某些场景下可能产生事实性错误
3. **上下文窗口**：尽管有优化，但极长上下文处理能力仍有局限
4. **推理深度**：在极其复杂的多步推理任务上仍有提升空间
5. **多语言支持**：主要优化中英文，其他语言支持相对有限
6. **蒸馏模型能力差异**：蒸馏版本虽然表现优异，但与原始大模型仍有能力差距
7. **特定配置要求**：DeepSeek-R1系列模型对温度和提示词格式有特定要求

## 未来发展

深度求索团队已透露的未来发展方向包括：
- 扩展更多专业领域的专门优化模型
- 增强多语言能力，尤其是非英语语言支持
- 持续改进长上下文理解能力
- 开发更高效的混合专家模型架构
- 增强工具使用和代码执行能力

## 参考资源

- [DeepSeek官方网站](https://deepseek.ai/)
- [DeepSeek GitHub仓库](https://github.com/deepseek-ai)
- [DeepSeek Hugging Face组织](https://huggingface.co/deepseek-ai)
- [DeepSeek-LLM论文](https://arxiv.org/abs/2401.02954)
- [DeepSeek-Coder论文](https://arxiv.org/abs/2401.14196)
- [DeepSeek-R1论文](https://arxiv.org/abs/2501.12948)
- [DeepSeek-R1-Distill-Llama-70B模型页面](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)
- [DeepSeek-R1-Distill-Qwen-32B模型页面](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) 