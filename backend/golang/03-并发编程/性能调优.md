# 性能调优

## 学习目标
- 掌握Go并发程序的性能分析方法
- 学习识别和解决性能瓶颈
- 理解并发程序的优化策略
- 掌握性能测试和基准测试技术

## 1. 性能分析基础

### pprof性能分析
```go
import (
    _ "net/http/pprof"
    "net/http"
    "log"
    "runtime"
    "time"
    "sync"
)

func enableProfiling() {
    // 启动pprof服务器
    go func() {
        log.Println(http.ListenAndServe("localhost:6060", nil))
    }()
    
    // 访问 http://localhost:6060/debug/pprof/ 查看性能信息
    // go tool pprof http://localhost:6060/debug/pprof/profile 进行CPU分析
    // go tool pprof http://localhost:6060/debug/pprof/heap 进行内存分析
}

func cpuIntensiveTask() {
    // CPU密集型任务示例
    var wg sync.WaitGroup
    
    for i := 0; i < runtime.NumCPU(); i++ {
        wg.Add(1)
        go func(workerID int) {
            defer wg.Done()
            
            start := time.Now()
            count := 0
            
            // 执行1秒的计算任务
            for time.Since(start) < time.Second {
                count++
                _ = count * count
            }
            
            fmt.Printf("Worker %d 完成 %d 次计算\n", workerID, count)
        }(i)
    }
    
    wg.Wait()
}

func memoryIntensiveTask() {
    // 内存密集型任务示例
    var slices [][]byte
    
    for i := 0; i < 1000; i++ {
        // 分配大块内存
        slice := make([]byte, 1024*1024) // 1MB
        
        // 填充数据
        for j := range slice {
            slice[j] = byte(i % 256)
        }
        
        slices = append(slices, slice)
        
        if i%100 == 0 {
            runtime.GC() // 手动触发GC
            var m runtime.MemStats
            runtime.ReadMemStats(&m)
            fmt.Printf("分配 %d MB, 堆大小: %d KB\n", i, m.HeapAlloc/1024)
        }
    }
    
    // 使用分配的内存
    for _, slice := range slices {
        _ = slice[0]
    }
}
```

### 运行时统计信息
```go
func runtimeStats() {
    ticker := time.NewTicker(1 * time.Second)
    defer ticker.Stop()
    
    for i := 0; i < 10; i++ {
        <-ticker.C
        
        var m runtime.MemStats
        runtime.ReadMemStats(&m)
        
        fmt.Printf("Goroutines: %d\n", runtime.NumGoroutine())
        fmt.Printf("内存分配: %d KB\n", m.Alloc/1024)
        fmt.Printf("系统内存: %d KB\n", m.Sys/1024)
        fmt.Printf("GC次数: %d\n", m.NumGC)
        fmt.Printf("暂停时间: %v\n", time.Duration(m.PauseTotalNs))
        fmt.Println("---")
    }
}

func goroutineLeakDetection() {
    before := runtime.NumGoroutine()
    fmt.Printf("开始时Goroutine数量: %d\n", before)
    
    // 启动一些可能泄露的goroutines
    for i := 0; i < 100; i++ {
        go func(id int) {
            // 模拟泄露的goroutine（永不退出）
            select {} // 永远阻塞
        }(i)
    }
    
    time.Sleep(100 * time.Millisecond)
    after := runtime.NumGoroutine()
    fmt.Printf("运行后Goroutine数量: %d\n", after)
    fmt.Printf("可能的Goroutine泄露: %d\n", after-before)
}
```

## 2. 并发性能优化

### Goroutine池优化
```go
type WorkerPool struct {
    workers     int
    jobQueue    chan Job
    resultQueue chan Result
    quit        chan bool
    wg          sync.WaitGroup
}

type Job struct {
    ID   int
    Data interface{}
}

type Result struct {
    JobID  int
    Output interface{}
    Error  error
    Duration time.Duration
}

func NewOptimizedWorkerPool(workers, jobQueueSize, resultQueueSize int) *WorkerPool {
    return &WorkerPool{
        workers:     workers,
        jobQueue:    make(chan Job, jobQueueSize),
        resultQueue: make(chan Result, resultQueueSize),
        quit:        make(chan bool),
    }
}

func (wp *WorkerPool) Start() {
    for i := 0; i < wp.workers; i++ {
        wp.wg.Add(1)
        go wp.worker(i)
    }
}

func (wp *WorkerPool) worker(id int) {
    defer wp.wg.Done()
    
    for {
        select {
        case job := <-wp.jobQueue:
            start := time.Now()
            result := wp.processJob(job)
            result.Duration = time.Since(start)
            
            select {
            case wp.resultQueue <- result:
            case <-wp.quit:
                return
            }
            
        case <-wp.quit:
            return
        }
    }
}

func (wp *WorkerPool) processJob(job Job) Result {
    // 模拟不同类型的工作负载
    switch job.Data.(type) {
    case "cpu":
        return wp.cpuBoundTask(job)
    case "io":
        return wp.ioBoundTask(job)
    case "memory":
        return wp.memoryBoundTask(job)
    default:
        return Result{
            JobID: job.ID,
            Error: fmt.Errorf("unknown job type"),
        }
    }
}

func (wp *WorkerPool) cpuBoundTask(job Job) Result {
    // CPU密集型任务
    iterations := 1000000
    sum := 0
    for i := 0; i < iterations; i++ {
        sum += i
    }
    
    return Result{
        JobID:  job.ID,
        Output: sum,
    }
}

func (wp *WorkerPool) ioBoundTask(job Job) Result {
    // I/O密集型任务
    time.Sleep(10 * time.Millisecond)
    
    return Result{
        JobID:  job.ID,
        Output: "I/O completed",
    }
}

func (wp *WorkerPool) memoryBoundTask(job Job) Result {
    // 内存密集型任务
    data := make([]int, 100000)
    for i := range data {
        data[i] = i * i
    }
    
    return Result{
        JobID:  job.ID,
        Output: len(data),
    }
}

func (wp *WorkerPool) Submit(job Job) {
    wp.jobQueue <- job
}

func (wp *WorkerPool) Results() <-chan Result {
    return wp.resultQueue
}

func (wp *WorkerPool) Stop() {
    close(wp.quit)
    wp.wg.Wait()
    close(wp.jobQueue)
    close(wp.resultQueue)
}

// 性能基准测试
func benchmarkWorkerPool() {
    workerCounts := []int{1, 2, 4, 8, 16}
    jobCount := 1000
    
    for _, workers := range workerCounts {
        fmt.Printf("\n测试 %d 个workers:\n", workers)
        
        start := time.Now()
        pool := NewOptimizedWorkerPool(workers, 100, 100)
        pool.Start()
        
        // 提交任务
        go func() {
            for i := 0; i < jobCount; i++ {
                job := Job{
                    ID:   i,
                    Data: "cpu", // CPU密集型任务
                }
                pool.Submit(job)
            }
        }()
        
        // 收集结果
        completed := 0
        totalDuration := time.Duration(0)
        
        for completed < jobCount {
            result := <-pool.Results()
            completed++
            totalDuration += result.Duration
        }
        
        pool.Stop()
        
        elapsed := time.Since(start)
        avgDuration := totalDuration / time.Duration(jobCount)
        
        fmt.Printf("总耗时: %v\n", elapsed)
        fmt.Printf("平均任务耗时: %v\n", avgDuration)
        fmt.Printf("吞吐量: %.2f jobs/sec\n", float64(jobCount)/elapsed.Seconds())
    }
}
```

### Channel性能优化
```go
// 带缓冲vs无缓冲channel性能对比
func benchmarkChannels() {
    const numMessages = 1000000
    
    // 测试无缓冲channel
    start := time.Now()
    unbufferedTest(numMessages)
    unbufferedTime := time.Since(start)
    
    // 测试不同大小的缓冲channel
    bufferSizes := []int{1, 10, 100, 1000, 10000}
    
    fmt.Printf("无缓冲channel: %v\n", unbufferedTime)
    
    for _, size := range bufferSizes {
        start = time.Now()
        bufferedTest(numMessages, size)
        bufferedTime := time.Since(start)
        
        fmt.Printf("缓冲大小 %d: %v (提升 %.2fx)\n", 
                   size, bufferedTime, 
                   float64(unbufferedTime)/float64(bufferedTime))
    }
}

func unbufferedTest(numMessages int) {
    ch := make(chan int)
    done := make(chan bool)
    
    // 发送者
    go func() {
        for i := 0; i < numMessages; i++ {
            ch <- i
        }
        close(ch)
    }()
    
    // 接收者
    go func() {
        for range ch {
            // 模拟少量处理
        }
        done <- true
    }()
    
    <-done
}

func bufferedTest(numMessages, bufferSize int) {
    ch := make(chan int, bufferSize)
    done := make(chan bool)
    
    // 发送者
    go func() {
        for i := 0; i < numMessages; i++ {
            ch <- i
        }
        close(ch)
    }()
    
    // 接收者
    go func() {
        for range ch {
            // 模拟少量处理
        }
        done <- true
    }()
    
    <-done
}
```

### 同步原语性能对比
```go
func benchmarkSynchronization() {
    const iterations = 1000000
    
    // Mutex性能测试
    start := time.Now()
    benchmarkMutex(iterations)
    mutexTime := time.Since(start)
    
    // RWMutex性能测试（读多写少）
    start = time.Now()
    benchmarkRWMutex(iterations)
    rwMutexTime := time.Since(start)
    
    // 原子操作性能测试
    start = time.Now()
    benchmarkAtomic(iterations)
    atomicTime := time.Since(start)
    
    // Channel性能测试
    start = time.Now()
    benchmarkChannelSync(iterations)
    channelTime := time.Since(start)
    
    fmt.Printf("Mutex: %v\n", mutexTime)
    fmt.Printf("RWMutex: %v\n", rwMutexTime)
    fmt.Printf("Atomic: %v (%.2fx faster than Mutex)\n", 
               atomicTime, float64(mutexTime)/float64(atomicTime))
    fmt.Printf("Channel: %v\n", channelTime)
}

func benchmarkMutex(iterations int) {
    var mu sync.Mutex
    var counter int
    var wg sync.WaitGroup
    
    workers := runtime.NumCPU()
    iterationsPerWorker := iterations / workers
    
    for i := 0; i < workers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for j := 0; j < iterationsPerWorker; j++ {
                mu.Lock()
                counter++
                mu.Unlock()
            }
        }()
    }
    
    wg.Wait()
}

func benchmarkRWMutex(iterations int) {
    var mu sync.RWMutex
    var counter int
    var wg sync.WaitGroup
    
    workers := runtime.NumCPU()
    iterationsPerWorker := iterations / workers
    
    // 主要是读操作（90%读，10%写）
    for i := 0; i < workers; i++ {
        wg.Add(1)
        go func(workerID int) {
            defer wg.Done()
            for j := 0; j < iterationsPerWorker; j++ {
                if j%10 == 0 { // 写操作
                    mu.Lock()
                    counter++
                    mu.Unlock()
                } else { // 读操作
                    mu.RLock()
                    _ = counter
                    mu.RUnlock()
                }
            }
        }(i)
    }
    
    wg.Wait()
}

func benchmarkAtomic(iterations int) {
    var counter int64
    var wg sync.WaitGroup
    
    workers := runtime.NumCPU()
    iterationsPerWorker := iterations / workers
    
    for i := 0; i < workers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for j := 0; j < iterationsPerWorker; j++ {
                atomic.AddInt64(&counter, 1)
            }
        }()
    }
    
    wg.Wait()
}

func benchmarkChannelSync(iterations int) {
    ch := make(chan struct{})
    var wg sync.WaitGroup
    
    // 一个worker负责计数
    wg.Add(1)
    go func() {
        defer wg.Done()
        counter := 0
        for i := 0; i < iterations; i++ {
            <-ch
            counter++
        }
    }()
    
    // 其他workers发送信号
    workers := runtime.NumCPU()
    iterationsPerWorker := iterations / workers
    
    for i := 0; i < workers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for j := 0; j < iterationsPerWorker; j++ {
                ch <- struct{}{}
            }
        }()
    }
    
    wg.Wait()
}
```

## 3. 内存优化

### 对象池模式
```go
type Buffer struct {
    data []byte
}

func (b *Buffer) Reset() {
    b.data = b.data[:0]
}

var bufferPool = sync.Pool{
    New: func() interface{} {
        return &Buffer{
            data: make([]byte, 0, 1024), // 预分配1KB
        }
    },
}

func getBuffer() *Buffer {
    return bufferPool.Get().(*Buffer)
}

func putBuffer(b *Buffer) {
    b.Reset()
    bufferPool.Put(b)
}

// 使用对象池vs每次分配的性能对比
func benchmarkObjectPool() {
    const iterations = 100000
    
    // 不使用对象池
    start := time.Now()
    for i := 0; i < iterations; i++ {
        buffer := make([]byte, 1024)
        _ = buffer
    }
    withoutPoolTime := time.Since(start)
    
    // 使用对象池
    start = time.Now()
    for i := 0; i < iterations; i++ {
        buffer := getBuffer()
        putBuffer(buffer)
    }
    withPoolTime := time.Since(start)
    
    fmt.Printf("不使用对象池: %v\n", withoutPoolTime)
    fmt.Printf("使用对象池: %v\n", withPoolTime)
    fmt.Printf("性能提升: %.2fx\n", float64(withoutPoolTime)/float64(withPoolTime))
}
```

### 内存分配优化
```go
// 预分配切片容量
func optimizeSliceAllocation() {
    const numItems = 100000
    
    // 不预分配容量（性能较差）
    start := time.Now()
    var slice1 []int
    for i := 0; i < numItems; i++ {
        slice1 = append(slice1, i)
    }
    withoutPreallocTime := time.Since(start)
    
    // 预分配容量（性能较好）
    start = time.Now()
    slice2 := make([]int, 0, numItems)
    for i := 0; i < numItems; i++ {
        slice2 = append(slice2, i)
    }
    withPreallocTime := time.Since(start)
    
    // 直接分配并设置值（最佳性能）
    start = time.Now()
    slice3 := make([]int, numItems)
    for i := 0; i < numItems; i++ {
        slice3[i] = i
    }
    directAssignTime := time.Since(start)
    
    fmt.Printf("不预分配: %v\n", withoutPreallocTime)
    fmt.Printf("预分配容量: %v\n", withPreallocTime)
    fmt.Printf("直接分配: %v\n", directAssignTime)
}

// 减少内存分配的字符串操作
func optimizeStringOperations() {
    words := []string{"Hello", "World", "Go", "Performance", "Optimization"}
    const iterations = 10000
    
    // 使用+操作符（性能较差）
    start := time.Now()
    for i := 0; i < iterations; i++ {
        var result string
        for _, word := range words {
            result += word + " "
        }
    }
    concatTime := time.Since(start)
    
    // 使用strings.Builder（性能较好）
    start = time.Now()
    for i := 0; i < iterations; i++ {
        var builder strings.Builder
        for _, word := range words {
            builder.WriteString(word)
            builder.WriteString(" ")
        }
        _ = builder.String()
    }
    builderTime := time.Since(start)
    
    // 使用预分配容量的strings.Builder（最佳性能）
    start = time.Now()
    for i := 0; i < iterations; i++ {
        var builder strings.Builder
        builder.Grow(50) // 预分配容量
        for _, word := range words {
            builder.WriteString(word)
            builder.WriteString(" ")
        }
        _ = builder.String()
    }
    optimizedBuilderTime := time.Since(start)
    
    fmt.Printf("字符串连接: %v\n", concatTime)
    fmt.Printf("StringBuilder: %v\n", builderTime)
    fmt.Printf("优化的StringBuilder: %v\n", optimizedBuilderTime)
}
```

## 4. I/O性能优化

### 批量I/O操作
```go
// 批量写入文件
func benchmarkFileIO() {
    const numWrites = 10000
    data := []byte("Hello, World!\n")
    
    // 单次写入
    start := time.Now()
    file1, _ := os.Create("single_writes.txt")
    for i := 0; i < numWrites; i++ {
        file1.Write(data)
    }
    file1.Close()
    singleWriteTime := time.Since(start)
    
    // 带缓冲的写入
    start = time.Now()
    file2, _ := os.Create("buffered_writes.txt")
    writer := bufio.NewWriter(file2)
    for i := 0; i < numWrites; i++ {
        writer.Write(data)
    }
    writer.Flush()
    file2.Close()
    bufferedWriteTime := time.Since(start)
    
    // 批量写入
    start = time.Now()
    file3, _ := os.Create("batch_writes.txt")
    var buffer bytes.Buffer
    for i := 0; i < numWrites; i++ {
        buffer.Write(data)
    }
    file3.Write(buffer.Bytes())
    file3.Close()
    batchWriteTime := time.Since(start)
    
    fmt.Printf("单次写入: %v\n", singleWriteTime)
    fmt.Printf("缓冲写入: %v\n", bufferedWriteTime)
    fmt.Printf("批量写入: %v\n", batchWriteTime)
    
    // 清理文件
    os.Remove("single_writes.txt")
    os.Remove("buffered_writes.txt")
    os.Remove("batch_writes.txt")
}

// 并发I/O操作
func concurrentFileProcessing(filenames []string) {
    // 控制并发数量
    semaphore := make(chan struct{}, runtime.NumCPU())
    var wg sync.WaitGroup
    
    for _, filename := range filenames {
        wg.Add(1)
        go func(name string) {
            defer wg.Done()
            
            // 获取信号量
            semaphore <- struct{}{}
            defer func() { <-semaphore }()
            
            // 处理文件
            processFile(name)
        }(filename)
    }
    
    wg.Wait()
}

func processFile(filename string) {
    file, err := os.Open(filename)
    if err != nil {
        return
    }
    defer file.Close()
    
    scanner := bufio.NewScanner(file)
    lineCount := 0
    
    for scanner.Scan() {
        lineCount++
        // 处理每一行
    }
    
    fmt.Printf("文件 %s 包含 %d 行\n", filename, lineCount)
}
```

## 5. 基准测试

### 编写基准测试
```go
// 基准测试示例
func BenchmarkChannelVsMutex(b *testing.B) {
    b.Run("Channel", func(b *testing.B) {
        ch := make(chan int, 1)
        ch <- 0
        
        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            val := <-ch
            val++
            ch <- val
        }
    })
    
    b.Run("Mutex", func(b *testing.B) {
        var mu sync.Mutex
        var val int
        
        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            mu.Lock()
            val++
            mu.Unlock()
        }
    })
    
    b.Run("Atomic", func(b *testing.B) {
        var val int64
        
        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            atomic.AddInt64(&val, 1)
        }
    })
}

func BenchmarkMemoryAllocation(b *testing.B) {
    b.Run("WithoutPool", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            buffer := make([]byte, 1024)
            _ = buffer
        }
    })
    
    b.Run("WithPool", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            buffer := getBuffer()
            putBuffer(buffer)
        }
    })
}

// 运行基准测试：
// go test -bench=. -benchmem
// -benchmem 显示内存分配统计
```

### 性能分析报告
```go
func generatePerformanceReport() {
    fmt.Println("=== 性能优化报告 ===")
    
    // CPU信息
    fmt.Printf("CPU核心数: %d\n", runtime.NumCPU())
    fmt.Printf("GOMAXPROCS: %d\n", runtime.GOMAXPROCS(0))
    
    // 内存信息
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    fmt.Printf("堆内存: %d KB\n", m.HeapAlloc/1024)
    fmt.Printf("系统内存: %d KB\n", m.Sys/1024)
    
    // Goroutine信息
    fmt.Printf("Goroutine数量: %d\n", runtime.NumGoroutine())
    
    // GC信息
    fmt.Printf("GC次数: %d\n", m.NumGC)
    fmt.Printf("GC暂停时间: %v\n", time.Duration(m.PauseTotalNs))
    
    // 运行基准测试
    fmt.Println("\n=== 基准测试结果 ===")
    benchmarkWorkerPool()
    benchmarkChannels()
    benchmarkSynchronization()
    benchmarkObjectPool()
}
```

## 6. 实践练习

### 练习1：优化HTTP服务器
```go
// 优化一个高并发的HTTP服务器
type OptimizedServer struct {
    // 实现连接池
    // 实现请求限流
    // 实现响应缓存
    // 优化内存分配
}

func (s *OptimizedServer) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    // 实现高性能的请求处理
}
```

### 练习2：优化数据处理管道
```go
// 优化一个数据处理管道
type DataPipeline struct {
    // 优化缓冲区大小
    // 实现并发处理
    // 减少内存分配
    // 优化I/O操作
}

func (dp *DataPipeline) Process(input <-chan []byte) <-chan []byte {
    // 实现高性能的数据处理
    return nil
}
```

### 练习3：优化缓存系统
```go
// 优化一个内存缓存系统
type OptimizedCache struct {
    // 优化并发访问
    // 实现LRU淘汰
    // 减少锁竞争
    // 优化内存使用
}

func (c *OptimizedCache) Get(key string) (interface{}, bool) {
    // 实现高性能的缓存获取
    return nil, false
}

func (c *OptimizedCache) Set(key string, value interface{}) {
    // 实现高性能的缓存设置
}
```

## 7. 性能监控

### 持续性能监控
```go
type PerformanceMonitor struct {
    mu           sync.RWMutex
    metrics      map[string]*Metric
    samplePeriod time.Duration
    done         chan bool
}

type Metric struct {
    Name    string
    Value   float64
    Unit    string
    Updated time.Time
}

func NewPerformanceMonitor(samplePeriod time.Duration) *PerformanceMonitor {
    pm := &PerformanceMonitor{
        metrics:      make(map[string]*Metric),
        samplePeriod: samplePeriod,
        done:         make(chan bool),
    }
    go pm.monitor()
    return pm
}

func (pm *PerformanceMonitor) monitor() {
    ticker := time.NewTicker(pm.samplePeriod)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            pm.collectMetrics()
        case <-pm.done:
            return
        }
    }
}

func (pm *PerformanceMonitor) collectMetrics() {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    
    pm.updateMetric("goroutines", float64(runtime.NumGoroutine()), "count")
    pm.updateMetric("heap_alloc", float64(m.HeapAlloc)/1024/1024, "MB")
    pm.updateMetric("heap_sys", float64(m.HeapSys)/1024/1024, "MB")
    pm.updateMetric("gc_count", float64(m.NumGC), "count")
    pm.updateMetric("gc_pause", float64(m.PauseTotalNs)/1000000, "ms")
}

func (pm *PerformanceMonitor) updateMetric(name string, value float64, unit string) {
    pm.mu.Lock()
    defer pm.mu.Unlock()
    
    pm.metrics[name] = &Metric{
        Name:    name,
        Value:   value,
        Unit:    unit,
        Updated: time.Now(),
    }
}

func (pm *PerformanceMonitor) GetMetrics() map[string]*Metric {
    pm.mu.RLock()
    defer pm.mu.RUnlock()
    
    result := make(map[string]*Metric)
    for k, v := range pm.metrics {
        result[k] = &Metric{
            Name:    v.Name,
            Value:   v.Value,
            Unit:    v.Unit,
            Updated: v.Updated,
        }
    }
    return result
}

func (pm *PerformanceMonitor) Stop() {
    close(pm.done)
}
```

## 8. 参考资料

- [Go性能优化指南](https://golang.org/doc/diagnostics.html)
- [pprof使用指南](https://golang.org/pkg/net/http/pprof/)
- [Go基准测试](https://golang.org/pkg/testing/#hdr-Benchmarks)
- [Go内存模型](https://golang.org/ref/mem)

---

通过本章的学习，你将掌握Go并发程序的性能分析和优化技术，能够识别性能瓶颈并进行针对性的优化，构建出高性能的并发应用程序。